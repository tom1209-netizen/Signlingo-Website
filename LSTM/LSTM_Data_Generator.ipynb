{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ3w8yzItmQ-"
      },
      "source": [
        "# Generate data for LSTM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INA-5NYkttL3"
      },
      "source": [
        "## Importing library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lZzf57OXtteF"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapipe as mp\n",
        "from sklearn.model_selection import train_test_split\n",
        "import copy\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc3gvQUatuth"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jIN-miEPtwVZ"
      },
      "outputs": [],
      "source": [
        "# DATA\n",
        "BATCH_SIZE = 32\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "INPUT_SHAPE = (30, 1086)\n",
        "NUM_CLASSES = 40\n",
        "\n",
        "# DATA PATH - GOOGLE DRIVE PATH\n",
        "VIDEO_DATASET_PATH = \"Videos\"\n",
        "LSTM_DATASET_PATH = \"LSTM_DATASET\"\n",
        "\n",
        "# MEDIAPIPE HOLISTIC\n",
        "POSE_NUM_LANDMARK = 33\n",
        "LEFT_HAND_NUM_LANDMARK = 21\n",
        "RIGHT_HAND_NUM_LANDMARK = 21\n",
        "FACE_NUM_LANDMARK = 468\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlWVnb-juADG"
      },
      "source": [
        "## Function to get mediapipe coordinate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "w2KyTsm_tznT"
      },
      "outputs": [],
      "source": [
        "mp_holistic = mp.solutions.holistic\n",
        "\n",
        "def apply_mediapipe_holistic(frame):\n",
        "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "        # Convert the frame to RGB as Mediapipe Holistic requires RGB input\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = holistic.process(image=frame_rgb)\n",
        "\n",
        "        # Initialize list to store the coordinates\n",
        "        coordinates = []\n",
        "\n",
        "        # Pose landmark\n",
        "        if results.pose_landmarks is not None:\n",
        "            for landmark in results.pose_landmarks.landmark:\n",
        "                coordinates.extend([landmark.x, landmark.y])\n",
        "        else:\n",
        "            # Add zeros for the pose landmarks if nothing is detected\n",
        "            num_pose_landmarks = POSE_NUM_LANDMARK * 2\n",
        "            coordinates.extend([0.0] * num_pose_landmarks)\n",
        "\n",
        "        # Left-hand landmark\n",
        "        if results.left_hand_landmarks is not None:\n",
        "            for landmark in results.left_hand_landmarks.landmark:\n",
        "                coordinates.extend([landmark.x, landmark.y])\n",
        "        else:\n",
        "            # Add zeros for the left-hand landmarks if nothing is detected\n",
        "            num_left_hand_landmarks = LEFT_HAND_NUM_LANDMARK * 2\n",
        "            coordinates.extend([0.0] * num_left_hand_landmarks)\n",
        "\n",
        "        # Right-hand landmark\n",
        "        if results.right_hand_landmarks is not None:\n",
        "            for landmark in results.right_hand_landmarks.landmark:\n",
        "                coordinates.extend([landmark.x, landmark.y])\n",
        "        else:\n",
        "            # Add zeros for the right-hand landmarks if nothing is detected\n",
        "            num_right_hand_landmarks = RIGHT_HAND_NUM_LANDMARK * 2\n",
        "            coordinates.extend([0.0] * num_right_hand_landmarks)\n",
        "\n",
        "        # Face landmark\n",
        "        if results.face_landmarks is not None:\n",
        "            for landmark in results.face_landmarks.landmark:\n",
        "                coordinates.extend([landmark.x, landmark.y])\n",
        "        else:\n",
        "            # Add zeros for the face landmarks if nothing is detected\n",
        "            num_face_landmarks = FACE_NUM_LANDMARK * 2\n",
        "            coordinates.extend([0.0] * num_face_landmarks)\n",
        "\n",
        "        return np.array(coordinates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eErzLn4uGH3"
      },
      "source": [
        "## Extract frame from video and covert to coordinate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Tw7N6K56t3Eo"
      },
      "outputs": [],
      "source": [
        "def extract_frames(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    required_frames = 30\n",
        "\n",
        "    # Calculate the frame interval to\n",
        "    # exactly 30 frames\n",
        "    frame_interval = max(1, total_frames // required_frames)\n",
        "\n",
        "    for frame_idx in range(0, total_frames, frame_interval):\n",
        "        if len(frames) >= required_frames:\n",
        "            break\n",
        "\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Apply mediapipe_holistic\n",
        "        coordinates = apply_mediapipe_holistic(frame)\n",
        "        frames.append(coordinates)\n",
        "\n",
        "    cap.release()\n",
        "    return np.array(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f8t_ZvsuZau"
      },
      "source": [
        "## Create data - test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "As3MatrX2NM9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
          ]
        }
      ],
      "source": [
        "# Ensure the new dataset directory exists\n",
        "if not os.path.exists(LSTM_DATASET_PATH):\n",
        "    os.makedirs(LSTM_DATASET_PATH)\n",
        "\n",
        "# Iterate through each class folder in the original dataset\n",
        "class_folders = os.listdir(VIDEO_DATASET_PATH)\n",
        "for class_folder in class_folders:\n",
        "    class_dir = os.path.join(VIDEO_DATASET_PATH, class_folder)\n",
        "\n",
        "    # Ensure it's a directory\n",
        "    if os.path.isdir(class_dir):\n",
        "        # Create a new folder for the class in the new dataset directory\n",
        "        new_class_dir = os.path.join(LSTM_DATASET_PATH, class_folder)\n",
        "        os.makedirs(new_class_dir, exist_ok=True)\n",
        "\n",
        "        # Get the list of video files in the class folder\n",
        "        video_files = [file for file in os.listdir(class_dir) if file.endswith(\".mp4\")]\n",
        "\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            futures = [executor.submit(extract_frames, os.path.join(class_dir, video_file))\n",
        "                           for video_file in video_files]\n",
        "\n",
        "            for future, video_file in zip(futures, video_files):\n",
        "                frames = future.result()\n",
        "\n",
        "                # Save the frames as a numpy array in the new dataset directory\n",
        "                np.save(os.path.join(new_class_dir, f\"{video_file.split('.')[0]}.npy\"), frames)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
